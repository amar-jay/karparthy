{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Paper]()\n",
    "\n",
    "- [ ] Activation functions\n",
    "- [ ] Batch Normalization. \n",
    "- [ ] Residual connections \n",
    "- [ ] the Adam optimizer\n",
    "- [ ] Backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a188f10e-7532-4b1e-a501-6f77aa31e31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n - [ ] Activation functions\\n - [ ] Batch Normalization. \\n - [ ] Residual connections \\n - [ ] the Adam optimizer\\n - [ ] Backpropagation\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    " - [ ] Activation functions\n",
    " - [ ] Batch Normalization. \n",
    " - [ ] Residual connections \n",
    " - [ ] the Adam optimizer\n",
    " - [ ] Backpropagation\n",
    "\"\"\"\n",
    "# [Paper]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "torch.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names:  ['cionna', 'abagail', 'malai', 'saathvik', 'paiton']\n",
      "number of names:  32033\n",
      "(list of chars, count):  ('.abcdefghijklmnopqrstuvwxyz', 27)\n",
      "(max word length, min word length):  (15, 2)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "with open(\"names.txt\", \"r+\") as f:\n",
    "\twords = f.read().splitlines()\n",
    "\twords = [word.strip() for word in words] # get rid of any trailing spaces\n",
    "\twords = [w for w in words if w] # get rid of any empty strings\n",
    "\tnames = sorted(words, key=lambda x: random.random())\n",
    "\n",
    "min_chars = min(len(v) for v in names)\n",
    "max_chars = max(len(v) for v in names)\n",
    "chars = sorted(list(set(\"\".join(names))))\n",
    "\n",
    "# in replacement of the start and end token. Every name should end with a period. and there should be no start token to begin a sequence\n",
    "chars = ['.'] + chars\n",
    "chars_count = len(chars)\n",
    "print(\"names: \", names[:5])\n",
    "print(\"number of names: \", len(names))\n",
    "print(\"(list of chars, count): \", (\"\".join(chars), chars_count))\n",
    "print(\"(max word length, min word length): \", (max_chars, min_chars))\n",
    "\n",
    "atoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itoa = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "# adding end token to each name\n",
    "names = [list(name) + ['.'] for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-params\n",
    "n_embd = 10\n",
    "block_size  = 3 # context length\n",
    "n_embd2 = 200 # intermediate weight size\n",
    "lr = 0.1 # determined based on graph\n",
    "decay_rate = 0.01\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... => c\n",
      "..c => i\n",
      ".ci => o\n",
      "cio => n\n",
      "ion => n\n"
     ]
    }
   ],
   "source": [
    "# build_dset basically builds a rolling window on the dataset based on the context length.\n",
    "def build_dset(dset, ctxt_len):\n",
    "    X, Y = [], []\n",
    "    for name in dset:\n",
    "        context  = [0] * ctxt_len\n",
    "        for ch in name:\n",
    "            ix = atoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            # print(''.join(itoa[i] for i in context), '--->', itoa[ix])\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "n1 = int(0.8*len(names))\n",
    "n2 = int(0.9*len(names))\n",
    "\n",
    "X_train, Y_train = build_dset(names[:n1], block_size)\n",
    "X_val, Y_val = build_dset(names[n1:n2], block_size)\n",
    "X_test, Y_test = build_dset(names[n2:], block_size)\n",
    "\n",
    "for c, d in zip(X_train[:5], Y_train[:5]):\n",
    "    print(''.join(itoa[i.item()] for i in c), \"=>\", itoa[d.item()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split(80, 10, 10)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "# parameters\n",
    "C = torch.randn((27, n_embd), generator=g)\n",
    "W1 = torch.randn((block_size * n_embd, n_embd2), generator=g) # hidden layer\n",
    "b1 = torch.randn(n_embd2, generator=g)\n",
    "W2 = torch.randn((n_embd2, 27), generator=g) # output layer\n",
    "b2 = torch.randn(27, generator=g)\n",
    "\n",
    "params = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in params:\n",
    "    p.requires_grad = True # autograd should record operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 0.0 ) loss =  24.17746925354004\n",
      "( 5.0 ) loss =  3.5246422290802\n",
      "( 10.0 ) loss =  2.8388516902923584\n",
      "( 15.0 ) loss =  2.8533453941345215\n",
      "( 20.0 ) loss =  2.642613410949707\n",
      "( 25.0 ) loss =  2.3912975788116455\n",
      "( 30.0 ) loss =  2.3728432655334473\n",
      "( 35.0 ) loss =  2.677912712097168\n",
      "( 40.0 ) loss =  2.1868112087249756\n",
      "( 45.0 ) loss =  2.750415086746216\n",
      "( 50.0 ) loss =  2.6992030143737793\n",
      "( 55.0 ) loss =  2.7639715671539307\n",
      "( 60.0 ) loss =  2.2791590690612793\n",
      "( 65.0 ) loss =  2.637249231338501\n",
      "( 70.0 ) loss =  2.218799114227295\n",
      "( 75.0 ) loss =  2.6497700214385986\n",
      "( 80.0 ) loss =  2.7670018672943115\n",
      "( 85.0 ) loss =  2.2003753185272217\n",
      "( 90.0 ) loss =  2.464599132537842\n",
      "( 95.0 ) loss =  2.5227200984954834\n"
     ]
    }
   ],
   "source": [
    "# BACKPROPAGATION\n",
    "\n",
    "max_steps = 20000\n",
    "losses = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct - for efficiency\n",
    "    ix = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[X_train[ix]].view(-1, 30)\n",
    "    v = emb @ W1 + b1\n",
    "    h = torch.tanh(v) # intermediate layer\n",
    "    logits = h @ W2 + b2\n",
    "        \n",
    "    loss = F.cross_entropy(logits, Y_train[ix])\n",
    "    if i % 1000 == 0:\n",
    "        print(\"(\", (i * 100) / max_steps , \") loss = \", loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lri = lr if max_steps > 10000 else decay_rate\n",
    "    for p in params:\n",
    "        p.data += - lri * p.grad \n",
    "    \n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  tensor(2.3842)\n",
      "test loss:  tensor(2.4027)\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def loss(x, y):\n",
    "    emb = C[x].view(-1, 30)\n",
    "    v = emb @ W1 + b1\n",
    "    h = torch.tanh(v) # intermediate layer\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    return loss\n",
    "\n",
    "print(\"training loss: \", loss(X_train, Y_train))\n",
    "print(\"test loss: \", loss(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats\n",
    "|iter| train | test | method |\n",
    "|--------|-----------| -------|---------|\n",
    "| 1 | 2.834 | 2.4027 | --- |\n",
    "| 2|  | | tuning output weights and bias - softmax | \n",
    "| 3|  | | tuning log params - tanh layer saturation | \n",
    "|4 | | | using kaiming init |\n",
    "|5 | | | add batch norm layer |\n",
    "| 6| | | running batch norm | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#fff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 1 | 2.834 | 2.4027 |  |\n",
    "| 2| | | tuning bias & weights | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
