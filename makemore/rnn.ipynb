{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f178b8f-f2e1-45cf-837d-dc69c47076f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1+cu121'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# hyper parameters\n",
    "LR = 0.01\n",
    "n_embd = 100 # size of hidden layer\n",
    "block_size = 20 # context length\n",
    "\n",
    "%matplotlib inline\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81cd0f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names:  ['pascual', 'santana', 'paraskevi', 'shane', 'aidan']\n",
      "number of names:  32033\n",
      "(list of chars, count):  ('.abcdefghijklmnopqrstuvwxyz', 27)\n",
      "(max word length, min word length):  (15, 1)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "with open(\"assets/names.txt\", \"r+\") as f:\n",
    "\twords = f.read().splitlines()\n",
    "\twords = [word.strip() for word in words] # get rid of any trailing spaces\n",
    "\tnames = [w for w in words if w] # get rid of any empty strings\n",
    "\t\n",
    "with open(\"assets/names.txt\", \"w\") as f: \n",
    "\tjoined = \"\\n\".join(names)\n",
    "\tf.write(joined)\n",
    "min_chars = 1\n",
    "max_chars = max(len(v) for v in names)\n",
    "chars = sorted(list(set(\"\".join(names))))\n",
    "\n",
    "# in replacement of the start and end token. Every name should end with a period. and there should be no start token to begin a sequence\n",
    "chars = ['.'] + chars\n",
    "vocab_size = len(chars)\n",
    "print(\"names: \", names[:5])\n",
    "print(\"number of names: \", len(names))\n",
    "print(\"(list of chars, count): \", (\"\".join(chars), vocab_size))\n",
    "print(\"(max word length, min word length): \", (max_chars, min_chars))\n",
    "\n",
    "atoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itoa = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "# adding end token to each name\n",
    "names = [list(name) + ['.'] for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "W1 = torch.randn(n_embd, vocab_size) # input to hidden\n",
    "W2 = torch.randn(n_embd, n_embd) # hidden to hidden \n",
    "W3 = torch.randn(vocab_size, n_embd) # hidden to output\n",
    "b1 = torch.zeros((n_embd, 1)) # bias for hidden layer\n",
    "b2 = torch.zeros((vocab_size, 1)) # bias for output layer\n",
    "\n",
    "# params = [W1, b1, W2, b2]\n",
    "# for p in params:\n",
    "# \tp.requires_grad = True\n",
    "\n",
    "# structure of the model\n",
    "# x -> embedding -> hidden layer -> tanh -> output -> softmax -> y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, y, hprev):\n",
    "\t\"\"\"\n",
    "\tx -> input\n",
    "\ty -> target\n",
    "\thprev -> hidden state from previous time step\n",
    "\t\"\"\"\n",
    "\txs, hs, ys, ps = {}, {}, {}\n",
    "\ths[-1] = hprev.copy()\n",
    "\tloss = 0\n",
    "\tfor i in range(len(x)):\n",
    "\t\txs[i] = torch.zeros((vocab_size, 1))\n",
    "\t\tx[i][x[t]] = 1\n",
    "\t\ths[i] = torch.tanh(torch.dot(W1, xs[i]) + torch.dot(W2, hs[i-1]) + b1)\n",
    "\t\tys[i] = F.softmax(torch.dot(W3, hs[i]) + b2, dim=0)\n",
    "\n",
    "\t\n",
    "def sample(h, seed_ix, n):\n",
    "\tx = torch.zeros((vocab_size, 1))\n",
    "\tx[seed_ix] = 1\n",
    "\tixes = []\n",
    "\tfor i in range(n):\n",
    "\t\tl1 = torch.dot(W1, x) + torch.dot(W2, h) + b1\n",
    "\t\tl2 = F.tanh(l1)\n",
    "\t\ty = torch.dot(W3, l2) + b2\n",
    "\t\tp = F.softmax(y, dim=0)\n",
    "\t\tix = torch.multinomial(p, num_samples=1, generator=g)\n",
    "\t\tx = torch.zeros((vocab_size, 0))\n",
    "\t\tx[ix] = 1\n",
    "\t\tixes.append(ix.item())\n",
    "\treturn ixes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:  hello\n"
     ]
    }
   ],
   "source": [
    "word = \"hello\"\n",
    "print(\"word: \", word)\n",
    "ixes = F.one_hot(torch.tensor([atoi[ch] for ch in word]), vocab_size).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 228145 characters, 27 unique.\n",
      "----\n",
      " hv\n",
      "crwrmrkmzxpgcsoyqhaopdvmwmptvgrctffrtrzospfmzyvrwsdjhukttflqulqzyxzqqnzfxsspjizjbanjoftgtkzqp\n",
      "tzizndwakopzgteezeywbvrzxslnozz\n",
      "kxbcvtplsdqqzuswv\n",
      "rurvrxkk\n",
      "efmdhlueijyeobycilj\n",
      "prpnmlerijxxsybpflhz\n",
      "brr \n",
      "----\n",
      "iter 0, loss: 82.395928\n",
      "----\n",
      " \n",
      "gf\n",
      "nz\n",
      "a\n",
      "dsaiknacl\n",
      "aua\n",
      "aiknk\n",
      "rianwua\n",
      "koa\n",
      "ammeg\n",
      "vianbnacu\n",
      "ahrnwirpa\n",
      "alaiaelij\n",
      "anagl\n",
      "lsamfiqia\n",
      "ldmwanayanfilvhekjluaskne\n",
      "acayaye\n",
      "tjkraianr\n",
      "piaiqiang\n",
      "lme\n",
      "aiailnazmlaya\n",
      "rvauailyanpezhaia\n",
      "ara\n",
      "a\n",
      "aql\n",
      "lesiaif \n",
      "----\n",
      "iter 100, loss: 83.431205\n",
      "----\n",
      " \n",
      "eiaiahehrray\n",
      "bhnnbntmt\n",
      "neouieh\n",
      "erbmmed\n",
      "nskkbhkliklnaslnudoera\n",
      "mh\n",
      "aaitashel\n",
      "ira\n",
      "eunii\n",
      "erisi\n",
      "eau\n",
      "jnluohe\n",
      "jxinahlnbn\n",
      "vi\n",
      "ie\n",
      "ebw\n",
      "ai\n",
      "qyosaebohr\n",
      "fhgos\n",
      "srakaj\n",
      "quhioap\n",
      "irkerny\n",
      "nay\n",
      "haln\n",
      "\n",
      "kllrdaird\n",
      "oeiilnbelabt \n",
      "----\n",
      "iter 200, loss: 82.803108\n",
      "----\n",
      " a\n",
      "yml\n",
      "anrramklan\n",
      "\n",
      "arb\n",
      "sha\n",
      "ardrny\n",
      "anrau\n",
      "riaieyanmaya\n",
      "ora\n",
      "alzylmar\n",
      "en\n",
      "lmliah\n",
      "hteamar\n",
      "mk\n",
      "daeet\n",
      "andrnma\n",
      "ame\n",
      "argy\n",
      "ljla\n",
      "\n",
      "yzrlmame\n",
      "ala\n",
      "iae\n",
      "\n",
      "lgynulclmi\n",
      "eeonamcvorle\n",
      "vkrakghanayeinjaalraginglrhuramaid\n",
      "wvbrarah \n",
      "----\n",
      "iter 300, loss: 81.915724\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/manan/code/karparthy/makemore/rnn.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m----\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m----\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (txt, ))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39m# forward seq_length characters through the net and fetch gradient\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m loss, dWxh, dWhh, dWhy, dbh, dby, hprev \u001b[39m=\u001b[39m lossFun(inputs, targets, hprev)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m smooth_loss \u001b[39m=\u001b[39m smooth_loss \u001b[39m*\u001b[39m \u001b[39m0.999\u001b[39m \u001b[39m+\u001b[39m loss \u001b[39m*\u001b[39m \u001b[39m0.001\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39miter \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, loss: \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (n, smooth_loss)) \u001b[39m# print progress\u001b[39;00m\n",
      "\u001b[1;32m/home/manan/code/karparthy/makemore/rnn.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m xs[t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((vocab_size,\u001b[39m1\u001b[39m)) \u001b[39m# encode in 1-of-k representation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m xs[t][inputs[t]] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m hs[t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtanh(np\u001b[39m.\u001b[39mdot(Wxh, xs[t]) \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39;49mdot(Whh, hs[t\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]) \u001b[39m+\u001b[39m bh) \u001b[39m# hidden state\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m ys[t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(Why, hs[t]) \u001b[39m+\u001b[39m by \u001b[39m# unnormalized log probabilities for next chars\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m ps[t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(ys[t]) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mexp(ys[t])) \u001b[39m# probabilities for next chars\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('assets/names.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
