{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f178b8f-f2e1-45cf-837d-dc69c47076f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1+cu121'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# hyper parameters\n",
    "LR = 0.01\n",
    "n_embd = 100 # size of hidden layer\n",
    "n_embd2 = 50 # size of hidden layer\n",
    "block_size = 8 # context length\n",
    "\n",
    "%matplotlib inline\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81cd0f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names:  ['nain', 'augustine', 'lanay', 'kalany', 'marijose']\n",
      "number of names:  32033\n",
      "(list of chars, count):  ('.abcdefghijklmnopqrstuvwxyz', 27)\n",
      "(max word length, min word length):  (15, 1)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "with open(\"assets/names.txt\", \"r+\") as f:\n",
    "\twords = f.read().splitlines()\n",
    "\twords = [word.strip() for word in words] # get rid of any trailing spaces\n",
    "\tnames = [w for w in words if w] # get rid of any empty strings\n",
    "\t\n",
    "with open(\"assets/names.txt\", \"w\") as f: \n",
    "\tjoined = \"\\n\".join(names)\n",
    "\tf.write(joined)\n",
    "min_chars = 1\n",
    "max_chars = max(len(v) for v in names)\n",
    "chars = sorted(list(set(\"\".join(names))))\n",
    "\n",
    "# in replacement of the start and end token. Every name should end with a period. and there should be no start token to begin a sequence\n",
    "chars = ['.'] + chars\n",
    "vocab_size = len(chars)\n",
    "print(\"names: \", names[:5])\n",
    "print(\"number of names: \", len(names))\n",
    "print(\"(list of chars, count): \", (\"\".join(chars), vocab_size))\n",
    "print(\"(max word length, min word length): \", (max_chars, min_chars))\n",
    "\n",
    "atoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itoa = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "# adding end token to each name\n",
    "names = [list(name) + ['.'] for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e873f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "    def __call__(self, x):\n",
    "        return self.weight[x]\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=None):\n",
    "        self.gain = torch.randn(\n",
    "            (in_features, out_features), dtype=dtype)\n",
    "        self.bias = torch.randn(\n",
    "            out_features, dtype=dtype) if bias else None\n",
    "\n",
    "    def __call__(self, input: torch.Tensor):\n",
    "        out = input @ self.gain\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gain] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a0a0fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "C = torch.randn(vocab_size, n_embd)\n",
    "Wxh = torch.randn(n_embd, n_embd + n_embd2) # input to hidden\n",
    "Whh = torch.randn(n_embd + n_embd2, n_embd2) # hidden to hidden \n",
    "Why = torch.randn(n_embd2, vocab_size) # hidden to output\n",
    "bh = torch.zeros(n_embd + n_embd2,) # bias for hidden layer\n",
    "by = torch.zeros(vocab_size,) # bias for output layer\n",
    "\n",
    "# hidden layer RNN states\n",
    "states = torch.zeros((1, n_embd))\n",
    "\n",
    "params = [C, W1, b1, W2, W3, b3]\n",
    "for p in params:\n",
    "\tp.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ced58de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ => n\n",
      ".......n => a\n",
      "......na => i\n",
      ".....nai => n\n",
      "....nain => .\n"
     ]
    }
   ],
   "source": [
    "# build_dset basically builds a rolling window on the dataset based on the context length.\n",
    "def build_dset(dset, ctxt_len):\n",
    "    X, Y = [], []\n",
    "    for name in dset:\n",
    "        context  = [0] * ctxt_len\n",
    "        for ch in name:\n",
    "            ix = atoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            # print(''.join(itoa[i] for i in context), '--->', itoa[ix])\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "n1 = int(0.8*len(names))\n",
    "n2 = int(0.9*len(names))\n",
    "\n",
    "X_train, Y_train = build_dset(names[:n1], block_size)\n",
    "X_val, Y_val = build_dset(names[n1:n2], block_size)\n",
    "X_test, Y_test = build_dset(names[n2:], block_size)\n",
    "\n",
    "for c, d in zip(X_train[:5], Y_train[:5]):\n",
    "    print(''.join(itoa[i.item()] for i in c), \"=>\", itoa[d.item()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b3925b6e-4a46-4054-a979-b1b73a3e0c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 100]), torch.Size([1000, 8, 150]), torch.Size([1000, 150]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rnn(x, y, hidden):\n",
    "    hidden = torch.tanh()\n",
    "    \n",
    "X= X_train[:1000]\n",
    "Y = Y_train[:1000]\n",
    "emb = C[X]\n",
    "l1 = emb @ W1\n",
    "l1 += b1\n",
    "hidden = torch.tanh(l1 + Whh @ hidden.pad(0) + bh)\n",
    "xh.shape\n",
    "# l2 = l1 @ W2\n",
    "# l3 = l2 @ W3\n",
    "# l3 += b3\n",
    "# logits = F.softmax(l3, dim=-1)\n",
    "states_0.shape, l1.shape\n",
    "# logits.sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "29551ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:  hello\n"
     ]
    }
   ],
   "source": [
    "word = \"hello\"\n",
    "print(\"word: \", word)\n",
    "ixes = F.one_hot(torch.tensor([atoi[ch] for ch in word]), vocab_size).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ef8e9460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 228145 characters, 27 unique.\n",
      "----\n",
      " hv\n",
      "crwrmrkmzxpgcsoyqhaopdvmwmptvgrctffrtrzospfmzyvrwsdjhukttflqulqzyxzqqnzfxsspjizjbanjoftgtkzqp\n",
      "tzizndwakopzgteezeywbvrzxslnozz\n",
      "kxbcvtplsdqqzuswv\n",
      "rurvrxkk\n",
      "efmdhlueijyeobycilj\n",
      "prpnmlerijxxsybpflhz\n",
      "brr \n",
      "----\n",
      "iter 0, loss: 82.395928\n",
      "----\n",
      " \n",
      "gf\n",
      "nz\n",
      "a\n",
      "dsaiknacl\n",
      "aua\n",
      "aiknk\n",
      "rianwua\n",
      "koa\n",
      "ammeg\n",
      "vianbnacu\n",
      "ahrnwirpa\n",
      "alaiaelij\n",
      "anagl\n",
      "lsamfiqia\n",
      "ldmwanayanfilvhekjluaskne\n",
      "acayaye\n",
      "tjkraianr\n",
      "piaiqiang\n",
      "lme\n",
      "aiailnazmlaya\n",
      "rvauailyanpezhaia\n",
      "ara\n",
      "a\n",
      "aql\n",
      "lesiaif \n",
      "----\n",
      "iter 100, loss: 83.431205\n",
      "----\n",
      " \n",
      "eiaiahehrray\n",
      "bhnnbntmt\n",
      "neouieh\n",
      "erbmmed\n",
      "nskkbhkliklnaslnudoera\n",
      "mh\n",
      "aaitashel\n",
      "ira\n",
      "eunii\n",
      "erisi\n",
      "eau\n",
      "jnluohe\n",
      "jxinahlnbn\n",
      "vi\n",
      "ie\n",
      "ebw\n",
      "ai\n",
      "qyosaebohr\n",
      "fhgos\n",
      "srakaj\n",
      "quhioap\n",
      "irkerny\n",
      "nay\n",
      "haln\n",
      "\n",
      "kllrdaird\n",
      "oeiilnbelabt \n",
      "----\n",
      "iter 200, loss: 82.803108\n",
      "----\n",
      " a\n",
      "yml\n",
      "anrramklan\n",
      "\n",
      "arb\n",
      "sha\n",
      "ardrny\n",
      "anrau\n",
      "riaieyanmaya\n",
      "ora\n",
      "alzylmar\n",
      "en\n",
      "lmliah\n",
      "hteamar\n",
      "mk\n",
      "daeet\n",
      "andrnma\n",
      "ame\n",
      "argy\n",
      "ljla\n",
      "\n",
      "yzrlmame\n",
      "ala\n",
      "iae\n",
      "\n",
      "lgynulclmi\n",
      "eeonamcvorle\n",
      "vkrakghanayeinjaalraginglrhuramaid\n",
      "wvbrarah \n",
      "----\n",
      "iter 300, loss: 81.915724\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/manan/code/karparthy/makemore/rnn.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m----\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m----\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (txt, ))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39m# forward seq_length characters through the net and fetch gradient\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m loss, dWxh, dWhh, dWhy, dbh, dby, hprev \u001b[39m=\u001b[39m lossFun(inputs, targets, hprev)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m smooth_loss \u001b[39m=\u001b[39m smooth_loss \u001b[39m*\u001b[39m \u001b[39m0.999\u001b[39m \u001b[39m+\u001b[39m loss \u001b[39m*\u001b[39m \u001b[39m0.001\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39miter \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, loss: \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (n, smooth_loss)) \u001b[39m# print progress\u001b[39;00m\n",
      "\u001b[1;32m/home/manan/code/karparthy/makemore/rnn.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m xs[t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((vocab_size,\u001b[39m1\u001b[39m)) \u001b[39m# encode in 1-of-k representation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m xs[t][inputs[t]] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m hs[t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtanh(np\u001b[39m.\u001b[39mdot(Wxh, xs[t]) \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39;49mdot(Whh, hs[t\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]) \u001b[39m+\u001b[39m bh) \u001b[39m# hidden state\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m ys[t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(Why, hs[t]) \u001b[39m+\u001b[39m by \u001b[39m# unnormalized log probabilities for next chars\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m ps[t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(ys[t]) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mexp(ys[t])) \u001b[39m# probabilities for next chars\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('assets/names.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter \n"
   ]
>>>>>>> 480f71d579182649a4ef3cd2a7828dabeb830b97
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
