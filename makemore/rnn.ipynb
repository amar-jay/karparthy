{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f178b8f-f2e1-45cf-837d-dc69c47076f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1+cu121'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# hyper parameters\n",
    "LR = 0.01\n",
    "n_embd = 100 # size of hidden layer\n",
    "n_embd2 = 50 # size of hidden layer\n",
    "block_size = 8 # context length\n",
    "\n",
    "%matplotlib inline\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3419759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names:  ['nain', 'augustine', 'lanay', 'kalany', 'marijose']\n",
      "number of names:  32033\n",
      "(list of chars, count):  ('.abcdefghijklmnopqrstuvwxyz', 27)\n",
      "(max word length, min word length):  (15, 1)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "with open(\"names.txt\", \"r+\") as f:\n",
    "\twords = f.read().splitlines()\n",
    "\twords = [word.strip() for word in words] # get rid of any trailing spaces\n",
    "\tnames = [w for w in words if w] # get rid of any empty strings\n",
    "\t\n",
    "with open(\"names.txt\", \"w\") as f: \n",
    "\tjoined = \"\\n\".join(names)\n",
    "\tf.write(joined)\n",
    "min_chars = 1\n",
    "max_chars = max(len(v) for v in names)\n",
    "chars = sorted(list(set(\"\".join(names))))\n",
    "\n",
    "# in replacement of the start and end token. Every name should end with a period. and there should be no start token to begin a sequence\n",
    "chars = ['.'] + chars\n",
    "vocab_size = len(chars)\n",
    "print(\"names: \", names[:5])\n",
    "print(\"number of names: \", len(names))\n",
    "print(\"(list of chars, count): \", (\"\".join(chars), vocab_size))\n",
    "print(\"(max word length, min word length): \", (max_chars, min_chars))\n",
    "\n",
    "atoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itoa = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "# adding end token to each name\n",
    "names = [list(name) + ['.'] for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ed1982-8f67-45ad-b3c6-8bf6a0860571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "    def __call__(self, x):\n",
    "        return self.weight[x]\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=None):\n",
    "        self.gain = torch.randn(\n",
    "            (in_features, out_features), dtype=dtype)\n",
    "        self.bias = torch.randn(\n",
    "            out_features, dtype=dtype) if bias else None\n",
    "\n",
    "    def __call__(self, input: torch.Tensor):\n",
    "        out = input @ self.gain\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gain] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78c25514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "C = torch.randn(vocab_size, n_embd)\n",
    "Wxh = torch.randn(n_embd, n_embd + n_embd2) # input to hidden\n",
    "Whh = torch.randn(n_embd + n_embd2, n_embd2) # hidden to hidden \n",
    "Why = torch.randn(n_embd2, vocab_size) # hidden to output\n",
    "bh = torch.zeros(n_embd + n_embd2,) # bias for hidden layer\n",
    "by = torch.zeros(vocab_size,) # bias for output layer\n",
    "\n",
    "# hidden layer RNN states\n",
    "states = torch.zeros((1, n_embd))\n",
    "\n",
    "params = [C, W1, b1, W2, W3, b3]\n",
    "for p in params:\n",
    "\tp.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ced58de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ => n\n",
      ".......n => a\n",
      "......na => i\n",
      ".....nai => n\n",
      "....nain => .\n"
     ]
    }
   ],
   "source": [
    "# build_dset basically builds a rolling window on the dataset based on the context length.\n",
    "def build_dset(dset, ctxt_len):\n",
    "    X, Y = [], []\n",
    "    for name in dset:\n",
    "        context  = [0] * ctxt_len\n",
    "        for ch in name:\n",
    "            ix = atoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            # print(''.join(itoa[i] for i in context), '--->', itoa[ix])\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "n1 = int(0.8*len(names))\n",
    "n2 = int(0.9*len(names))\n",
    "\n",
    "X_train, Y_train = build_dset(names[:n1], block_size)\n",
    "X_val, Y_val = build_dset(names[n1:n2], block_size)\n",
    "X_test, Y_test = build_dset(names[n2:], block_size)\n",
    "\n",
    "for c, d in zip(X_train[:5], Y_train[:5]):\n",
    "    print(''.join(itoa[i.item()] for i in c), \"=>\", itoa[d.item()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b3925b6e-4a46-4054-a979-b1b73a3e0c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 100]), torch.Size([1000, 8, 150]), torch.Size([1000, 150]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rnn(x, y, hidden):\n",
    "    hidden = torch.tanh()\n",
    "    \n",
    "X= X_train[:1000]\n",
    "Y = Y_train[:1000]\n",
    "emb = C[X]\n",
    "l1 = emb @ W1\n",
    "l1 += b1\n",
    "hidden = torch.tanh(l1 + Whh @ hidden.pad(0) + bh)\n",
    "xh.shape\n",
    "# l2 = l1 @ W2\n",
    "# l3 = l2 @ W3\n",
    "# l3 += b3\n",
    "# logits = F.softmax(l3, dim=-1)\n",
    "states_0.shape, l1.shape\n",
    "# logits.sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b318ffe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:  hello\n"
     ]
    }
   ],
   "source": [
    "word = \"hello\"\n",
    "print(\"word: \", word)\n",
    "ixes = F.one_hot(torch.tensor([atoi[ch] for ch in word]), vocab_size).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5ce9cc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 228145 characters, 27 unique.\n",
      "----\n",
      " hv\n",
      "crwrmrkmzxpgcsoyqhaopdvmwmptvgrctffrtrzospfmzyvrwsdjhukttflqulqzyxzqqnzfxsspjizjbanjoftgtkzqp\n",
      "tzizndwakopzgteezeywbvrzxslnozz\n",
      "kxbcvtplsdqqzuswv\n",
      "rurvrxkk\n",
      "efmdhlueijyeobycilj\n",
      "prpnmlerijxxsybpflhz\n",
      "brr \n",
      "----\n",
      "iter 0, loss: 82.395928\n",
      "----\n",
      " \n",
      "gf\n",
      "nz\n",
      "a\n",
      "dsaiknacl\n",
      "aua\n",
      "aiknk\n",
      "rianwua\n",
      "koa\n",
      "ammeg\n",
      "vianbnacu\n",
      "ahrnwirpa\n",
      "alaiaelij\n",
      "anagl\n",
      "lsamfiqia\n",
      "ldmwanayanfilvhekjluaskne\n",
      "acayaye\n",
      "tjkraianr\n",
      "piaiqiang\n",
      "lme\n",
      "aiailnazmlaya\n",
      "rvauailyanpezhaia\n",
      "ara\n",
      "a\n",
      "aql\n",
      "lesiaif \n",
      "----\n",
      "iter 100, loss: 83.431205\n",
      "----\n",
      " \n",
      "eiaiahehrray\n",
      "bhnnbntmt\n",
      "neouieh\n",
      "erbmmed\n",
      "nskkbhkliklnaslnudoera\n",
      "mh\n",
      "aaitashel\n",
      "ira\n",
      "eunii\n",
      "erisi\n",
      "eau\n",
      "jnluohe\n",
      "jxinahlnbn\n",
      "vi\n",
      "ie\n",
      "ebw\n",
      "ai\n",
      "qyosaebohr\n",
      "fhgos\n",
      "srakaj\n",
      "quhioap\n",
      "irkerny\n",
      "nay\n",
      "haln\n",
      "\n",
      "kllrdaird\n",
      "oeiilnbelabt \n",
      "----\n",
      "iter 200, loss: 82.803108\n",
      "----\n",
      " a\n",
      "yml\n",
      "anrramklan\n",
      "\n",
      "arb\n",
      "sha\n",
      "ardrny\n",
      "anrau\n",
      "riaieyanmaya\n",
      "ora\n",
      "alzylmar\n",
      "en\n",
      "lmliah\n",
      "hteamar\n",
      "mk\n",
      "daeet\n",
      "andrnma\n",
      "ame\n",
      "argy\n",
      "ljla\n",
      "\n",
      "yzrlmame\n",
      "ala\n",
      "iae\n",
      "\n",
      "lgynulclmi\n",
      "eeonamcvorle\n",
      "vkrakghanayeinjaalraginglrhuramaid\n",
      "wvbrarah \n",
      "----\n",
      "iter 300, loss: 81.915724\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/manan/code/karparthy/makemore/rnn.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m----\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m----\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (txt, ))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39m# forward seq_length characters through the net and fetch gradient\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m loss, dWxh, dWhh, dWhy, dbh, dby, hprev \u001b[39m=\u001b[39m lossFun(inputs, targets, hprev)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m smooth_loss \u001b[39m=\u001b[39m smooth_loss \u001b[39m*\u001b[39m \u001b[39m0.999\u001b[39m \u001b[39m+\u001b[39m loss \u001b[39m*\u001b[39m \u001b[39m0.001\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39miter \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, loss: \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (n, smooth_loss)) \u001b[39m# print progress\u001b[39;00m\n",
      "\u001b[1;32m/home/manan/code/karparthy/makemore/rnn.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m xs[t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((vocab_size,\u001b[39m1\u001b[39m)) \u001b[39m# encode in 1-of-k representation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m xs[t][inputs[t]] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m hs[t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtanh(np\u001b[39m.\u001b[39mdot(Wxh, xs[t]) \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39;49mdot(Whh, hs[t\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]) \u001b[39m+\u001b[39m bh) \u001b[39m# hidden state\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m ys[t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(Why, hs[t]) \u001b[39m+\u001b[39m by \u001b[39m# unnormalized log probabilities for next chars\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/manan/code/karparthy/makemore/rnn.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m ps[t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(ys[t]) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mexp(ys[t])) \u001b[39m# probabilities for next chars\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
