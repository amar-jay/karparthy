import unicodedata



class Tokenizer:
    def __init__(self, lang):
        self.lang = lang
    def train(self, text):
        pass
    def encode(self, text):
        pass
    def decode(self, tokens):
        pass

    def _build_vocab(self, text):
        pass

    def save(self, path):
        pass
    def load(self, path):
        pass
