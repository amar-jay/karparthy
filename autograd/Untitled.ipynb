{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b60a71-7bb5-4af3-acfd-ce50c0ca0254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not equal True\n",
      "not equal True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.ones((3,3))\n",
    "B = np.triu((3,3, 3))\n",
    "\n",
    "C = A + B\n",
    "\n",
    "dC = np.ones_like(C)\n",
    "dA = np.sum(dC, axis=0)\n",
    "dB = dC\n",
    "\n",
    "# dA.shape == A.shape, dB.shape == B.shape,\n",
    "A.shape == C.shape\n",
    "for i, j in zip(A.shape, C.shape):\n",
    "    print(\"not equal\", i == j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "521c02b2-174e-4b38-8f20-f82a95216a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 1D array\n",
    "arr = np.linspace(0, 100, 101)\n",
    "arr = arr[:-1]\n",
    "reshaped_arr = arr.reshape((2, 5, -1))\n",
    "arr = np.ones(10)\n",
    "# Reshape the array to a 2D array with 2 rows and 3 columns\n",
    "\n",
    "(reshaped_arr + arr).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f4614a9-d641-4a6a-a7bd-b4ae39ea4f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 10]), torch.Size([10, 5]), torch.Size([5, 5]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.ones((5, 10), requires_grad=True)\n",
    "\n",
    "B = torch.ones((10, 5))\n",
    "B = torch.triu(B)\n",
    "B.requires_grad=True\n",
    "\n",
    "A.retain_grad()\n",
    "B.retain_grad()\n",
    "\n",
    "C = A @ B\n",
    "grad_output = torch.ones_like(C)\n",
    "C.backward(grad_output, retain_graph=True)\n",
    "\n",
    "C.shape\n",
    "dA = grad_output @ B.T\n",
    "dB = A.T @ grad_output\n",
    "(dA == A.grad).all(), (dB == B.grad).all()\n",
    "A.shape, B.shape, C.shape\n",
    "# WHAT OF TENSORS GREATER THAN 2 DIMENSIONS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e60995ba-64fa-4c3b-946c-c3115470a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import numpy as np\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, arr=[], _children=set(), _backward=lambda:None):\n",
    "        if not isinstance(arr, np.ndarray):\n",
    "            if isinstance(arr, list):\n",
    "                arr = np.array(arr)\n",
    "            else:\n",
    "                raise ValueError(f'data should be of type \"numpy.ndarray\" or a scalar,but received {type(arr)}')\n",
    "\n",
    "        self.data = arr\n",
    "\n",
    "        self.dtype = self.dtype\n",
    "        self._children = _children\n",
    "        self._backward = _backward\n",
    "        self.grad = np.zeros_like(self.data, dtype=np.float64)  # is this really the best way to implement this?\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.grad = np.ones_like(self.data, dtype=np.float64)\n",
    "\n",
    "    def __add__(self, other:'Tensor'):\n",
    "        y = Tensor(self.data + other.data, (self, other))\n",
    "\n",
    "        def _backward():\n",
    "            if self.data.shape == other.data.shape:\n",
    "                self.grad += y.grad\n",
    "                other.grad += y.grad\n",
    "            else:\n",
    "                print(\"broadcasting of sizes\", self.data.shape, other.data.shape)\n",
    "                for i, j in zip(self.data.shape, y.data.shape):\n",
    "                    if i != j:\n",
    "                        raise ValueError(f\"Shapes are different self:{self.data.shape} other:{other.data.shape}\")\n",
    "\n",
    "                if len(self.data.shape) < len(y.data.shape):\n",
    "                    self.grad += np.sum(y.data, axis=0)\n",
    "                    other.grad += y.data\n",
    "\n",
    "        y._backward = _backward\n",
    "\n",
    "        return y\n",
    "\n",
    "    def __mul__(self, other:Union['Tensor', int, float]) -> 'Tensor':\n",
    "        \"\"\"\n",
    "            dot and scalar product\n",
    "        \"\"\"\n",
    "        if isinstance(other, (int, float)):\n",
    "            other = Tensor([other])\n",
    "            y = Tensor(other.data*self.data, (self,other))\n",
    "        else:\n",
    "            if self.data.shape != other.data.shape:\n",
    "                raise ValueError(f\"Shapes are different self:{self.data.shape} other:{other.data.shape}\")\n",
    "            y = Tensor(self.data * other.data, (self, other))\n",
    "\n",
    "        def _backward():\n",
    "            if isinstance(other, (int, float)):\n",
    "                self.grad += other * y.grad\n",
    "                return\n",
    "\n",
    "\n",
    "            if other.data.shape == (1, ):\n",
    "                self.grad += other.data * y.grad\n",
    "                return\n",
    "\n",
    "            if self.data.shape == other.data.shape:\n",
    "                self.grad += other.data * y.grad # works for two dimensional but fails for the rest\n",
    "                other.grad += self.data * y.grad\n",
    "            else:\n",
    "                raise ValueError(f\"Shapes are different self:{self.data.shape} other:{other.data.shape}\")\n",
    "\n",
    "                # raise NotImplementedError # understanding how matrix multiplcation works\n",
    "\n",
    "            return\n",
    "        y._backward = _backward\n",
    "        return y\n",
    "\n",
    "    def __matmul__(self, other: 'Tensor'):\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise ValueError(f'data should be of type \"Tensor\"  {type(other)}')\n",
    "        if self.data.shape != other.data.shape:\n",
    "            raise ValueError(f\"Shapes are different self:{self.data.shape} other:{other.data.shape}\")\n",
    "        y = Tensor(self.data @ other.data, (self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += np.dot(y.grad, other.grad.T)\n",
    "            other.grad += np.dot(self.grad.T, y.grad)\n",
    "        y._backward = _backward\n",
    "        return y\n",
    "\n",
    "    def __pow__(self, n):\n",
    "        if n < 0: # numpy does not support negative exponents\n",
    "            y = Tensor(1/(self.data ** -n), (self,))\n",
    "        else:\n",
    "            y = Tensor(self.data ** n, (self,))\n",
    "\n",
    "        def _backward():\n",
    "            if n-1 < 0: # numpy does not support negative exponents\n",
    "                self.grad += ((n / self.data ** -(n-1))) * y.grad\n",
    "            else:\n",
    "                self.grad += (n * self.data ** (n-1)) * y.grad\n",
    "        y._backward = _backward\n",
    "        return y\n",
    "\n",
    "    def __div__(self, other:Union['Tensor', int, float]): # other / self\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "        return other * self**-1\n",
    "\n",
    "    def T(self):\n",
    "        y = Tensor(self.data.T, (self,))\n",
    "        def _backward():\n",
    "            self.grad += y.grad.T\n",
    "\n",
    "        y._backward = _backward\n",
    "        return y\n",
    "\n",
    "    def backward(self):\n",
    "        children = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(node):\n",
    "            if node not in visited and node is not None:\n",
    "                visited.add(node)\n",
    "                if node._children:\n",
    "                    for child in node._children:\n",
    "                        build_topo(child)\n",
    "                children.append(node)\n",
    "        build_topo(self)\n",
    "\n",
    "        children.reverse()\n",
    "\n",
    "        for child in children:\n",
    "\n",
    "            child._backward()\n",
    "        return\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other:'Tensor'):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other:'Tensor'): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other:'Tensor'): # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def dtype(self, _dtype):\n",
    "        return self.data.astype(_dtype)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        data = self.data\n",
    "        grad = self.grad\n",
    "\n",
    "        return f\"Tensor<{data.tolist()}, {grad=}>\" if self.grad>0 else f\"Tensor<{data.tolist()}>\"\n",
    "\n",
    "\n",
    "def exp(x:Tensor):\n",
    "    y = np.exp(x.data)\n",
    "    y = Tensor(y, (x,))\n",
    "\n",
    "    def _backward():\n",
    "        dy = np.exp(x.data)\n",
    "        x.grad += dy * y.grad\n",
    "        return\n",
    "\n",
    "    y._backward = _backward\n",
    "    return y\n",
    "\n",
    "def log(x:Tensor):\n",
    "    y = np.log(x.data)\n",
    "    y = Tensor(y, (x,))\n",
    "    def _backward():\n",
    "        dy = x.data ** -1\n",
    "        x.grad += dy * y.grad\n",
    "        return\n",
    "\n",
    "    y._backward = _backward\n",
    "    return y\n",
    "\n",
    "# ---------------------------------- Activation functions --------------------------------\n",
    "\n",
    "def relu(x:Tensor):\n",
    "    y = np.maximum(x.data, 0)\n",
    "    y = Tensor(y, (x,))\n",
    "\n",
    "    def _backward():\n",
    "        x.grad[x.data>0] += y.grad[x.data>0]\n",
    "        return\n",
    "\n",
    "    y._backward = _backward\n",
    "    return y\n",
    "\n",
    "\n",
    "# checking if both implementation are equal\n",
    "def sigmoid_2(x:Tensor):\n",
    "    return (Tensor([1])+exp(-x)) ** -1\n",
    "\n",
    "def sigmoid(x:Tensor):\n",
    "    y = 1/(1+np.exp(-x.data))\n",
    "    y = Tensor(y, (x,))\n",
    "\n",
    "    def _backward():\n",
    "        dy = x.data*(1-x.data)\n",
    "        x.grad += dy * y.grad\n",
    "        return\n",
    "\n",
    "    y._backward = _backward\n",
    "    return y\n",
    "\n",
    "def tanh_2(x:Tensor): # this implementation through seems attractive leads to overflow error\n",
    "    return (exp(x) - exp(-x))/(exp(x) + exp(-x))\n",
    "\n",
    "def tanh(x:Tensor):\n",
    "    y = np.tanh(x.data)\n",
    "    y = Tensor(y, (x,))\n",
    "\n",
    "    def _backward():\n",
    "        dy = (1-y.data**2)\n",
    "        x.grad += dy * y.grad\n",
    "        return\n",
    "\n",
    "    y._backward = _backward\n",
    "    return y\n",
    "\n",
    "def sum_(x:Tensor, axis=None): # sums all elements\n",
    "    if axis:\n",
    "        raise NotImplementedError\n",
    "    y = x.data.sum()\n",
    "    y = np.array(y)\n",
    "    y = Tensor(y, (x,))\n",
    "    \n",
    "    def _backward():\n",
    "        x.grad += np.ones_like(x) * y.grad\n",
    "        return\n",
    "\n",
    "    y._backward = _backward\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1b729ad9-8596-4866-ac86-7c41c64ffe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.]), array([1.]), array([0.78643562]), array([-7.76674749e-18]), array([-1.74751819e-09]), array([-7.76674749e-18]), array([-7.76674749e-18]), array([7.76674749e-18]), array([-7.76674749e-18]), array([-2.33002425e-13]), array([0.]), array([-2.33002425e-12]), array([0.]), array([-1.16501212e-11]), array([-1.16501212e-11]), array([-1.16501212e-11])]\n"
     ]
    }
   ],
   "source": [
    "# from autograd import Tensor, log, sigmoid\n",
    "import numpy as np\n",
    "a = Tensor([100.0])\n",
    "\n",
    "b = Tensor([200.0])\n",
    "c = a + b\n",
    "d = Tensor([5.0])\n",
    "\n",
    "e = c * d\n",
    "f = Tensor([0.1])\n",
    "g = e / f\n",
    "h = g ** 2\n",
    "i  = -h\n",
    "j = Tensor([0.9])\n",
    "k = j-i\n",
    "l = log(k)\n",
    "m = exp(l)\n",
    "n = sigmoid(m ** -0.5)\n",
    "o = tanh(n)\n",
    "p = relu(o)\n",
    "\n",
    "params = [p, o,n, m,l,k,j,i, h,g,f,e,d,c,b,a]\n",
    "p.zero_grad()\n",
    "p.backward()\n",
    "print([i.grad for i in params])\n",
    "autograd_grads = [i.grad for i in params]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5236458c-1263-4053-85f4-5afb8228b2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1.]), tensor([1.]), tensor([0.7864]), tensor([-2.9127e-14]), tensor([-6.5536e-06]), tensor([-2.9127e-14]), tensor([-2.9127e-14]), tensor([2.9127e-14]), tensor([-2.9127e-14]), tensor([-8.7382e-10]), tensor([0.0001]), tensor([-8.7382e-09]), tensor([-2.6215e-06]), tensor([-4.3691e-08]), tensor([-4.3691e-08]), tensor([-4.3691e-08])]\n",
      "[0.00013107]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "a = tensor([100.0], requires_grad=True)\n",
    "b = tensor([200.0], requires_grad=True)\n",
    "c = a + b\n",
    "d = tensor([5.0], requires_grad=True)\n",
    "e = c * d\n",
    "f = tensor([.1], requires_grad=True)\n",
    "g = e / f\n",
    "h = g ** 2\n",
    "i  = -h\n",
    "j = tensor([0.9], requires_grad=True)\n",
    "k = j-i\n",
    "l = torch.log(k)\n",
    "m = torch.exp(l)\n",
    "n = torch.sigmoid(m ** -0.5)\n",
    "o = torch.tanh(n)\n",
    "p = torch.relu(o)\n",
    "params = [p, o,n, m,l,k,j,i, h,g,f,e,d,c,b,a]\n",
    "\n",
    "for i in params:\n",
    "    i.retain_grad()\n",
    "p.backward()\n",
    "print([i.grad for i in params])\n",
    "torch_grads = [i.grad.numpy() for i in params]\n",
    "\n",
    "for t, a in zip(torch_grads,autograd_grads):\n",
    "    if np.abs((t - a)) > 1e-5:\n",
    "        print(t-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "678c5224-22ec-4409-a6cd-486a816784e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no broadcasting here\n",
    "import numpy as np\n",
    "a = Tensor(np.ones((3,3)))\n",
    "\n",
    "b = Tensor(np.ones((3,3)))\n",
    "\n",
    "c = a + b\n",
    "d = Tensor(np.ones((3,3)))\n",
    "\n",
    "e = c * d\n",
    "f = Tensor(np.ones((3,3)))\n",
    "g = e / f\n",
    "h = g ** 2\n",
    "i  = -h\n",
    "j = Tensor(np.ones((3,3)))\n",
    "# k = j-i\n",
    "# l = log(k)\n",
    "# m = exp(l)\n",
    "# n = sigmoid(m ** -0.5)\n",
    "# o = tanh(n)\n",
    "# p = relu(o)\n",
    "q = sum_(j)\n",
    "\n",
    "params = [\n",
    "    q, \n",
    "    #p, o,n,m,l,k,\n",
    "    j,i, h,g,f,e,d,c, b,a]\n",
    "params[0].zero_grad()\n",
    "params[0].backward()\n",
    "autograd_grads = [i.grad for i in params]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c7715c3f-5ed0-44ed-8965-54632d92eb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., dtype=torch.float64)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([tensor(1.),\n",
       "  tensor([[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]]),\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " 11)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------------------ Torch -----------------------------\n",
    "import torch\n",
    "\n",
    "a = torch.ones(3,3, requires_grad = True)\n",
    "b = torch.ones(3,3, requires_grad = True)\n",
    "\n",
    "\n",
    "c = a + b\n",
    "d = torch.ones(3,3, requires_grad = True)\n",
    "\n",
    "e = c + d\n",
    "f = torch.ones(3,3, requires_grad = True)\n",
    "\n",
    "g = e / f\n",
    "h = g ** 2\n",
    "i  = -h\n",
    "j = torch.ones(3,3, requires_grad = True)\n",
    "\n",
    "# k = j-i\n",
    "# l = torch.log(k)\n",
    "# m = torch.exp(l)\n",
    "# n = torch.sigmoid(m ** -0.5)\n",
    "# o = torch.tanh(n)\n",
    "# p = torch.relu(o)\n",
    "q = torch.sum(j)\n",
    "\n",
    "params = [\n",
    "    # q, p, o,n,m,l,k,j,\n",
    "    q, j,i, h,g,f,e,d,c, b,a]\n",
    "\n",
    "params[0].grad = None\n",
    "for i in params:\n",
    "    i.retain_grad()\n",
    "\n",
    "params[0].backward()\n",
    "torch_grads = [i.grad for i in params]\n",
    "\n",
    "for t, a in zip(torch_grads,autograd_grads):\n",
    "\n",
    "    # if t is None: # resolve this\n",
    "    #     print(\"- \", a)\n",
    "    #     continue\n",
    "    if t is not None:\n",
    "        print(t-a)\n",
    "torch_grads, len(autograd_grads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
