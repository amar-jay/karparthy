{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b60a71-7bb5-4af3-acfd-ce50c0ca0254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not equal True\n",
      "not equal True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.ones((3,3))\n",
    "B = np.triu((3,3, 3))\n",
    "\n",
    "C = A + B\n",
    "\n",
    "dC = np.ones_like(C)\n",
    "dA = np.sum(dC, axis=0)\n",
    "dB = dC\n",
    "\n",
    "# dA.shape == A.shape, dB.shape == B.shape,\n",
    "A.shape == C.shape\n",
    "for i, j in zip(A.shape, C.shape):\n",
    "    print(\"not equal\", i == j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "521c02b2-174e-4b38-8f20-f82a95216a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 1D array\n",
    "arr = np.linspace(0, 100, 101)\n",
    "arr = arr[:-1]\n",
    "reshaped_arr = arr.reshape((2, 5, -1))\n",
    "arr = np.ones(10)\n",
    "# Reshape the array to a 2D array with 2 rows and 3 columns\n",
    "\n",
    "(reshaped_arr + arr).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f4614a9-d641-4a6a-a7bd-b4ae39ea4f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 10]), torch.Size([10, 5]), torch.Size([5, 5]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.ones((5, 10), requires_grad=True)\n",
    "\n",
    "B = torch.ones((10, 5))\n",
    "B = torch.triu(B)\n",
    "B.requires_grad=True\n",
    "\n",
    "A.retain_grad()\n",
    "B.retain_grad()\n",
    "\n",
    "C = A @ B\n",
    "grad_output = torch.ones_like(C)\n",
    "C.backward(grad_output, retain_graph=True)\n",
    "\n",
    "C.shape\n",
    "dA = grad_output @ B.T\n",
    "dB = A.T @ grad_output\n",
    "(dA == A.grad).all(), (dB == B.grad).all()\n",
    "A.shape, B.shape, C.shape\n",
    "# WHAT OF TENSORS GREATER THAN 2 DIMENSIONS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e60995ba-64fa-4c3b-946c-c3115470a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import numpy as np\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, arr=[], _children=set(), _backward=lambda:None):\n",
    "        if not isinstance(arr, np.ndarray):\n",
    "            if isinstance(arr, list):\n",
    "                arr = np.array(arr)\n",
    "            else:\n",
    "                raise ValueError(f'data should be of type \"numpy.ndarray\" or a scalar,but received {type(arr)}')\n",
    "\n",
    "        self.data = arr\n",
    "\n",
    "        self.dtype = self.dtype\n",
    "        self._children = _children\n",
    "        self._backward = _backward\n",
    "        self.grad = np.zeros_like(self.data, dtype=np.float64)  # is this really the best way to implement this?\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.grad = np.ones_like(self.data, dtype=np.float64)\n",
    "\n",
    "    def __add__(self, other:'Tensor'):\n",
    "        y = Tensor(self.data + other.data, (self, other))\n",
    "\n",
    "        def _backward():\n",
    "            if self.data.shape == other.data.shape:\n",
    "                self.grad += y.grad\n",
    "                other.grad += y.grad\n",
    "            else:\n",
    "                print(\"broadcasting of sizes\", self.data.shape, other.data.shape)\n",
    "                for i, j in zip(self.data.shape, y.data.shape):\n",
    "                    if i != j:\n",
    "                        raise ValueError(f\"Shapes are different self:{self.data.shape} other:{other.data.shape}\")\n",
    "\n",
    "                if len(self.data.shape) < len(y.data.shape):\n",
    "                    self.grad += np.sum(y.data, axis=0)\n",
    "                    other.grad += y.data\n",
    "\n",
    "        y._backward = _backward\n",
    "\n",
    "        return y\n",
    "\n",
    "    def __mul__(self, other:Union['Tensor', int, float]) -> 'Tensor':\n",
    "        \"\"\"\n",
    "            dot and scalar product\n",
    "        \"\"\"\n",
    "        if isinstance(other, (int, float)):\n",
    "            other = Tensor([other], dtype=s)\n",
    "            y = Tensor(other.data*self.data, (self,other))\n",
    "        else:\n",
    "            if self.data.shape != other.data.shape:\n",
    "                raise ValueError(f\"Shapes are different self:{self.data.shape} other:{other.data.shape}\")\n",
    "            y = Tensor(self.data * other.data, (self, other))\n",
    "\n",
    "        def _backward():\n",
    "            if isinstance(other, (int, float)):\n",
    "                self.grad += other * y.grad\n",
    "                return\n",
    "\n",
    "            if self.data.shape == other.data.shape:\n",
    "                self.grad += other.data * y.grad # works for two dimensional but fails for the rest\n",
    "                other.grad += self.data * y.grad\n",
    "            else:\n",
    "                raise NotImplementedError # understanding how matrix multiplcation works\n",
    "\n",
    "            return\n",
    "        y._backward = _backward\n",
    "        return y\n",
    "\n",
    "    def __matmul__(self, other: 'Tensor'):\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise ValueError(f'data should be of type \"Tensor\"  {type(other)}')\n",
    "        if self.data.shape != other.data.shape:\n",
    "            raise ValueError(f\"Shapes are different self:{self.data.shape} other:{other.data.shape}\")\n",
    "        y = Tensor(self.data @ other.data, (self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += np.dot(y.grad, other.grad.T)\n",
    "            other.grad += np.dot(self.grad.T, y.grad)\n",
    "        y._backward = _backward\n",
    "        return y\n",
    "\n",
    "    def __pow__(self, n):\n",
    "        if n < 0: # numpy does not support negative exponents\n",
    "            y = Tensor(1/(self.data ** -n), (self,))\n",
    "        else:\n",
    "            y = Tensor(self.data ** n, (self,))\n",
    "\n",
    "        def _backward():\n",
    "            if n-1 < 0: # numpy does not support negative exponents\n",
    "                self.grad += ((n / self.data ** -(n-1))) * y.grad\n",
    "            else:\n",
    "                self.grad += (n * self.data ** (n-1)) * y.grad\n",
    "        y._backward = _backward\n",
    "        return y\n",
    "\n",
    "    def __div__(self, other:Union['Tensor', int, float]): # other / self\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "        return other * self**-1\n",
    "\n",
    "    def T(self):\n",
    "        y = Tensor(self.data.T, (self,))\n",
    "        def _backward():\n",
    "            self.grad += y.grad.T\n",
    "\n",
    "        y._backward = _backward\n",
    "        return y\n",
    "\n",
    "    def backward(self):\n",
    "        children = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(node):\n",
    "            if node not in visited and node is not None:\n",
    "                visited.add(node)\n",
    "                if node._children:\n",
    "                    for child in node._children:\n",
    "                        build_topo(child)\n",
    "                children.append(node)\n",
    "        build_topo(self)\n",
    "\n",
    "        children.reverse()\n",
    "\n",
    "        for child in children:\n",
    "\n",
    "            child._backward()\n",
    "        return\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other:'Tensor'):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other:'Tensor'): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other:'Tensor'): # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def dtype(self, _dtype):\n",
    "        return self.data.astype(_dtype)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        data = self.data\n",
    "        grad = self.grad\n",
    "\n",
    "        return f\"Tensor<{data.tolist()}, {grad=}>\" if self.grad>0 else f\"Tensor<{data.tolist()}>\"\n",
    "\n",
    "\n",
    "# ---------------------------------- Activation functions --------------------------------\n",
    "def exp(x:Tensor):\n",
    "    y = np.exp(x.data)\n",
    "    y = Tensor(y, (x,))\n",
    "\n",
    "    def _backward():\n",
    "        dy = np.exp(x.data)\n",
    "        x.grad += dy * y.grad\n",
    "        return\n",
    "\n",
    "    y._backward = _backward\n",
    "    return y\n",
    "\n",
    "def log(x:Tensor):\n",
    "    y = np.log(x.data)\n",
    "    y = Tensor(y, (x,))\n",
    "    def _backward():\n",
    "        dy = x.data ** -1\n",
    "        x.grad += dy * y.grad\n",
    "        return\n",
    "\n",
    "    y._backward = _backward\n",
    "    return y\n",
    "\n",
    "def relu(x:Tensor):\n",
    "    y = np.maximum(x.data, 0)\n",
    "    y = Tensor(y, (x,))\n",
    "\n",
    "    def _backward():\n",
    "        x.grad[x.data>0] += y.grad[x.data>0]\n",
    "        return\n",
    "\n",
    "    y._backward = _backward\n",
    "    return y\n",
    "\n",
    "\n",
    "# check if both implementation are equal\n",
    "def sigmoid(x:Tensor):\n",
    "    return (Tensor([1])+exp(-x)) ** -1\n",
    "\n",
    "def sigmoid_2(x:Tensor):\n",
    "    y = 1/(1+np.exp(-x.data))\n",
    "    y = Tensor(y, (x,))\n",
    "\n",
    "    def _backward():\n",
    "        dy = x.data*(1-x.data)\n",
    "        x.grad += dy * y.grad\n",
    "        return\n",
    "\n",
    "    y._backward = _backward\n",
    "    return y\n",
    "\n",
    "def tanh(x:Tensor):\n",
    "    return (exp(x) - exp(-x))/(exp(x) + exp(-x))\n",
    "\n",
    "def tanh_2(x:Tensor):\n",
    "    y = np.tanh(x.data)\n",
    "    y = Tensor(y, (x,))\n",
    "\n",
    "    def _backward():\n",
    "        dy = (1-y.data**2)\n",
    "        x.grad += dy * y.grad\n",
    "        return\n",
    "\n",
    "    y._backward = _backward\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b729ad9-8596-4866-ac86-7c41c64ffe60",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Tensor.__init__() got an unexpected keyword argument 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m g \u001b[38;5;241m=\u001b[39m e \u001b[38;5;241m/\u001b[39m f\n\u001b[1;32m     12\u001b[0m h \u001b[38;5;241m=\u001b[39m g \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 13\u001b[0m i  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m-\u001b[39;49m\u001b[43mh\u001b[49m\n\u001b[1;32m     14\u001b[0m j \u001b[38;5;241m=\u001b[39m Tensor([\u001b[38;5;241m0.9\u001b[39m])\n\u001b[1;32m     15\u001b[0m k \u001b[38;5;241m=\u001b[39m j\u001b[38;5;241m-\u001b[39mi\n",
      "Cell \u001b[0;32mIn[56], line 131\u001b[0m, in \u001b[0;36mTensor.__neg__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__neg__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "Cell \u001b[0;32mIn[56], line 48\u001b[0m, in \u001b[0;36mTensor.__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    dot and scalar product\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)):\n\u001b[0;32m---> 48\u001b[0m     other \u001b[38;5;241m=\u001b[39m \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mother\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     y \u001b[38;5;241m=\u001b[39m Tensor(other\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, (\u001b[38;5;28mself\u001b[39m,other))\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Tensor.__init__() got an unexpected keyword argument 'dtype'"
     ]
    }
   ],
   "source": [
    "# from autograd import Tensor, log, sigmoid\n",
    "import numpy as np\n",
    "a = Tensor([100.0])\n",
    "\n",
    "b = Tensor([200.0])\n",
    "c = a + b\n",
    "d = Tensor([5.0])\n",
    "\n",
    "e = c * d\n",
    "f = Tensor([0.1])\n",
    "g = e / f\n",
    "h = g ** 2\n",
    "i  = -h\n",
    "j = Tensor([0.9])\n",
    "k = j-i\n",
    "l = log(k)\n",
    "m = exp(l)\n",
    "n = sigmoid(m)\n",
    "# issues with sigmoid\n",
    "# working; +, *, /, **, \n",
    "# c.children\n",
    "i.zero_grad()\n",
    "params = [n, m,l,k,j,i,h,g,f,e,d,c,b,a]\n",
    "i.backward()\n",
    "print([i.grad for i in params])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5236458c-1263-4053-85f4-5afb8228b2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None, tensor([1.]), tensor([30000.]), tensor([-4.5000e+09]), tensor([300000.]), tensor([90000000.]), tensor([1500000.]), tensor([1500000.]), tensor([1500000.])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "a = tensor([100.0], requires_grad=True)\n",
    "b = tensor([200.0], requires_grad=True)\n",
    "c = a + b\n",
    "d = tensor([5.0], requires_grad=True)\n",
    "e = c * d\n",
    "f = tensor([.1], requires_grad=True)\n",
    "g = e / f\n",
    "h = g ** 2\n",
    "i  = -h\n",
    "j = tensor([0.9], requires_grad=True)\n",
    "k = j-i\n",
    "l = torch.log(k)\n",
    "m = torch.exp(l)\n",
    "n = torch.sigmoid(m)\n",
    "params = [n, m,l,k,j,i, h,g,f,e,d,c,b,a]\n",
    "\n",
    "for i in params:\n",
    "    i.retain_grad()\n",
    "h.backward()\n",
    "print([i.grad for i in params])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
